{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I took the dataset taxi_data.h5 from Baidu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: Feb-22th-2023\n",
    "# Description: This source code creates hetergogenous graphs for taxi deamnd and supply prediciton \n",
    "# (with 3 edge types) and then uses them to train and test with GNN and LSTM models for predicting the demand \n",
    "# and suply values\n",
    "# by: Mahmoud Nazzal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "from torch_geometric.data import HeteroData\n",
    "# # resources:\n",
    "# 1.https://pytorch-geometric.readthedocs.io/en/2.0.0/notes/heterogeneous.html?highlight=HeteroGNN#using-the-heterogenous-convolution-wrapper \n",
    "# 2.https://levelup.gitconnected.com/forecasting-walmart-quarterly-revenue-pytorch-lstm-example-b4e4b20862a7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this routine converts adjacency matrices into adjacency lists\n",
    "def A_to_edge_index(A):\n",
    "    G=nx.from_numpy_matrix(A)\n",
    "    edge_index=list(G.edges)\n",
    "    z=torch.tensor(np.transpose(edge_index))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a GPU is aviaable, use it, o.w., use the CPU\n",
    "cuda_device = 0\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.set_device(cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 1. load the data from the pytorch files:\n",
    "# adj=torch.load('adj.pt')\n",
    "# adj2=adj[2]\n",
    "adj_0=torch.load('adj_0.pt')\n",
    "adj_1=torch.load('adj_1.pt')\n",
    "adj_2=torch.load('adj_2.pt')\n",
    "\n",
    "# adj_0=adj_0[0:10, 0:10]\n",
    "# adj_1=adj_1[0:10, 0:10]\n",
    "# adj_2=adj_2[0:10, 0:10]\n",
    "\n",
    "# 2. convert the adj  matrixes to adjacency lists\n",
    "edge_list0=A_to_edge_index(adj_0)\n",
    "edge_list1=A_to_edge_index(adj_1)\n",
    "edge_list2=A_to_edge_index(adj_2)\n",
    "# 3. load the training and testing node feature matrices\n",
    "data0=torch.load('data0.pt')\n",
    "data1=torch.load('data1.pt')\n",
    "X_train_new=torch.load('X_train_new.pt')\n",
    "X_test_new=torch.load('X_test_new.pt')\n",
    "# print(X_train_new.shape)\n",
    "# print(X_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.16438753 0.         ... 0.         0.         0.        ]\n",
      " [0.16438753 1.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.92816226 0.89872131]\n",
      " [0.         0.         0.         ... 0.92816226 1.         0.98325835]\n",
      " [0.         0.         0.         ... 0.89872131 0.98325835 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(adj_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([266, 216, 180])\n",
      "torch.Size([266, 216, 180])\n"
     ]
    }
   ],
   "source": [
    "# X_train_new=X_train_new[0:10,:,:]\n",
    "# X_test_new=X_test_new[0:10,:,:]\n",
    "\n",
    "print(X_train_new.shape)\n",
    "print(X_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.matlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain data for the cars in the first 10 regions\n",
    "arrz=[]\n",
    "for kk in range(100):\n",
    "    arr = np.empty((0,216))\n",
    "    arr_tru=np.empty((0,216))\n",
    "    t_s=kk\n",
    "    for i in range(40):\n",
    "        n_nodes_in_reg=np.sum(data1[t_s, i])\n",
    "\n",
    "        message_nodes_in_reg=X_train_new[i,:, t_s]\n",
    "        X_temp=np.matlib.repmat(message_nodes_in_reg, n_nodes_in_reg, 1)\n",
    "        arr = np.vstack((arr, X_temp))\n",
    "\n",
    "        message_nodes_in_reg_tru=X_train_new[i,:,t_s+1]\n",
    "        X_temp2=np.matlib.repmat(message_nodes_in_reg_tru, n_nodes_in_reg, 1)\n",
    "        arr_tru = np.vstack((arr_tru, X_temp2))\n",
    "\n",
    "    # X=np.asarray(X)   \n",
    "#     print(arr.shape)\n",
    "    arrz.append(arr.shape[0])\n",
    "#     print(arr_tru.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(arrz)\n",
    "# # hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# hist_array, bin_array = np.histogram(np.array(arrz))\n",
    "# # Set some configurations for the chart\n",
    "# # plt.figure(figsize=[10, 5])\n",
    "# plt.xlim(min(bin_array), max(bin_array))\n",
    "# # plt.grid(axis='y', alpha=0.75)\n",
    "# plt.xlabel('Edge Values', fontsize=20)\n",
    "# plt.ylabel('Histogram Values', fontsize=20)\n",
    "# plt.title('Histogram Chart', fontsize=25)\n",
    "\n",
    "# # Create the chart\n",
    "# # plt.bar(bin_array[:-1], hist_array, width=0.5, color='blue')\n",
    "# # Display the chart\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1916, 1916)\n",
      "torch.Size([2, 77583])\n"
     ]
    }
   ],
   "source": [
    "# obtain adj info for the cars in the first 10 regions\n",
    "t_s=0\n",
    "n_nodes_in_reg_list=[]\n",
    "A = np.empty((0,0))\n",
    "for i in range(40):\n",
    "        n_nodes_in_reg=np.sum(data1[t_s, i])\n",
    "        block=np.matlib.repmat(adj_0[i,i], n_nodes_in_reg, n_nodes_in_reg);\n",
    "        A=block_diag(A, block)\n",
    "A=A.astype(float)       \n",
    "print(A.shape)\n",
    "\n",
    "edge_list0=A_to_edge_index(A)\n",
    "print(edge_list0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(A[0:10,0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_per_ts(t_s):\n",
    "# obtain data for the cars in the first 10 regions\n",
    "    arr = np.empty((0,216))\n",
    "    arr_tru=np.empty((0,216))\n",
    "#     t_s=0\n",
    "    for i in range(40):\n",
    "        n_nodes_in_reg=np.sum(data1[t_s, i])\n",
    "\n",
    "        message_nodes_in_reg=X_train_new[i,:, t_s]\n",
    "        X_temp=np.matlib.repmat(message_nodes_in_reg, n_nodes_in_reg, 1)\n",
    "        arr = np.vstack((arr, X_temp))\n",
    "\n",
    "        message_nodes_in_reg_tru=X_train_new[i,:,t_s+1]\n",
    "        X_temp2=np.matlib.repmat(message_nodes_in_reg_tru, n_nodes_in_reg, 1)\n",
    "        arr_tru = np.vstack((arr_tru, X_temp2))\n",
    "\n",
    "    # X=np.asarray(X)   \n",
    "#     print(arr.shape)\n",
    "#     print(arr_tru.shape)  \n",
    "\n",
    "\n",
    "    # obtain adj info for the cars in the first 10 regions\n",
    "#     t_s=0\n",
    "    n_nodes_in_reg_list=[]\n",
    "    A0 = np.empty((0,0))\n",
    "    A1 = np.empty((0,0))\n",
    "    A2 = np.empty((0,0))\n",
    "    for i in range(40):\n",
    "            n_nodes_in_reg=np.sum(data1[t_s, i])\n",
    "            block0=np.matlib.repmat(adj_0[i,i], n_nodes_in_reg, n_nodes_in_reg);\n",
    "            A0=block_diag(A0, block0)\n",
    "            \n",
    "            block1=np.matlib.repmat(adj_0[i,i], n_nodes_in_reg, n_nodes_in_reg);\n",
    "            A1=block_diag(A1, block1)\n",
    "            \n",
    "            block2=np.matlib.repmat(adj_0[i,i], n_nodes_in_reg, n_nodes_in_reg);\n",
    "            A2=block_diag(A2, block2)\n",
    "    \n",
    "#     A0=A0.astype(float) \n",
    "#     print(A.shape)\n",
    "\n",
    "    edge_list0=A_to_edge_index(A0)\n",
    "    edge_list1=A_to_edge_index(A1)\n",
    "    edge_list2=A_to_edge_index(A2)\n",
    "#     print(edge_list0.shape)\n",
    "#     print(edge_list1.shape)\n",
    "#     print(edge_list2.shape)\n",
    "    data = HeteroData()\n",
    "    \n",
    "    \n",
    "    arr=torch.from_numpy(arr)\n",
    "    mean1, std1 =arr.mean(axis=0), arr.std(axis=0)\n",
    "    data['taxi'].x =  (arr - mean1) / std1\n",
    "    \n",
    "    \n",
    "    arr_tru=torch.from_numpy(arr_tru)\n",
    "    mean2, std2 =arr_tru.mean(axis=0), arr_tru.std(axis=0)\n",
    "    data['taxi'].y =  (arr_tru - mean2) / std2\n",
    "    \n",
    "    \n",
    "#     data['taxi'].x= arr\n",
    "    data['taxi', 'near', 'taxi'].edge_index = edge_list0\n",
    "    data['taxi', 'connected', 'taxi'].edge_index = edge_list1\n",
    "    data['taxi', 'OD_similar', 'taxi'].edge_index = edge_list2\n",
    "    temp=np.array([True])\n",
    "    temp2=np.tile(temp, 266)\n",
    "    data['taxi'].test_mask=torch.from_numpy(temp2)\n",
    "#     data['taxi'].y=torch.from_numpy(arr_tru)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=get_graph_per_ts(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('taxi', 'near', 'taxi'): tensor([[   0,    0,    0,  ..., 1914, 1914, 1915],\n",
      "        [   0,    1,    2,  ..., 1914, 1915, 1915]]), ('taxi', 'connected', 'taxi'): tensor([[   0,    0,    0,  ..., 1914, 1914, 1915],\n",
      "        [   0,    1,    2,  ..., 1914, 1915, 1915]]), ('taxi', 'OD_similar', 'taxi'): tensor([[   0,    0,    0,  ..., 1914, 1914, 1915],\n",
      "        [   0,    1,    2,  ..., 1914, 1915, 1915]])}\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a Heterogeneous Graph (with three edge types), as a \"data\" object\n",
    "# from torch_geometric.data import HeteroData\n",
    "# data = HeteroData()\n",
    "# data['taxi'].x= arr \n",
    "# data['taxi', 'near', 'taxi'].edge_index = edge_list0\n",
    "# data['taxi', 'connected', 'taxi'].edge_index = edge_list0\n",
    "# data['taxi', 'OD_similar', 'taxi'].edge_index = edge_list0\n",
    "# temp=np.array([True])\n",
    "# temp2=np.tile(temp, 266)\n",
    "# data['taxi'].test_mask=torch.from_numpy(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bring_ts_data():\n",
    "#     # obtain data for the cars in the first 10 regions\n",
    "#     arr = np.empty((0,216))\n",
    "#     arr_tru=np.empty((0,216))\n",
    "#     t_s=6\n",
    "#     for i in range(20):\n",
    "#         n_nodes_in_reg=np.sum(data1[t_s, i])\n",
    "\n",
    "#         message_nodes_in_reg=X_train_new[i,:, t_s]\n",
    "#         X_temp=np.matlib.repmat(message_nodes_in_reg, n_nodes_in_reg, 1)\n",
    "#         arr = np.vstack((arr, X_temp))\n",
    "\n",
    "#         message_nodes_in_reg_tru=X_train_new[i,:,t_s+1]\n",
    "#         X_temp2=np.matlib.repmat(message_nodes_in_reg_tru, n_nodes_in_reg, 1)\n",
    "#         arr_tru = np.vstack((arr_tru, X_temp2))\n",
    "\n",
    "#     # X=np.asarray(X)   \n",
    "#     print(arr_tru.shape)\n",
    "#     print(arr_tru.shape)   \n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize the data\n",
    "# mean0, std0 = X_train_new[:, :, 0].mean(axis=0), X_train_new[:, :, 0] .std(axis=0)\n",
    "# X_train_new[:, :, 0]  = (X_train_new[:, :, 0]  - mean0) / std0\n",
    "# X_train_new[:, :, 1] =  (X_train_new[:, :, 1]  - mean0) / std0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mtaxi\u001b[0m={\n",
      "    x=[1916, 216],\n",
      "    y=[1916, 216],\n",
      "    test_mask=[266]\n",
      "  },\n",
      "  \u001b[1m(taxi, near, taxi)\u001b[0m={ edge_index=[2, 77583] },\n",
      "  \u001b[1m(taxi, connected, taxi)\u001b[0m={ edge_index=[2, 77583] },\n",
      "  \u001b[1m(taxi, OD_similar, taxi)\u001b[0m={ edge_index=[2, 77583] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# verify the graph properties\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node types: ['taxi']\n",
      "Edge types: [('taxi', 'near', 'taxi'), ('taxi', 'connected', 'taxi'), ('taxi', 'OD_similar', 'taxi')]\n"
     ]
    }
   ],
   "source": [
    "# extract the metadata from the \"data\" object\n",
    "node_types, edge_types = data.metadata()\n",
    "print(\"Node types:\", node_types)\n",
    "print(\"Edge types:\", edge_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('taxi', 'near', 'taxi'): GCNConv(-1, hidden_channels),\n",
    "                ('taxi', 'connected', 'taxi'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('taxi', 'OD_similar', 'taxi'): GATConv((-1, -1), hidden_channels),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['taxi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialize a sample of the GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data['taxi'].x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroGNN(\n",
      "  (convs): ModuleList(\n",
      "    (0): HeteroConv(num_relations=3)\n",
      "    (1): HeteroConv(num_relations=3)\n",
      "    (2): HeteroConv(num_relations=3)\n",
      "  )\n",
      "  (lin): Linear(300, 216, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Model_GNN = HeteroGNN(hidden_channels=300, out_channels=216,\n",
    "                  num_layers=3)\n",
    "Model_GNN = Model_GNN.double()\n",
    "\n",
    "Model_GNN=Model_GNN.to(device)\n",
    "data=data.to(device)\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    Model_GNN.eval()\n",
    "    out = Model_GNN(data.x_dict, data.edge_index_dict)    \n",
    "    \n",
    "print(Model_GNN)# print a model summary\n",
    "# print(out.shape)# print the shape of outputs with this initial model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1916, 216])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1916\n"
     ]
    }
   ],
   "source": [
    "print(data['taxi'].x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtain training samples:\n",
    "# from torch_geometric.data import HeteroData\n",
    "# data = HeteroData()\n",
    "# data['taxi'].x= X_train_new[:, :, 0] \n",
    "# data['taxi', 'near', 'taxi'].edge_index = edge_list0\n",
    "# data['taxi', 'connected', 'taxi'].edge_index = edge_list2\n",
    "# data['taxi', 'OD_similar', 'taxi'].edge_index = edge_list2\n",
    "temp=np.array([True])\n",
    "temp2=np.tile(temp, data['taxi'].x.shape[0])\n",
    "data['taxi'].train_mask=torch.from_numpy(temp2)\n",
    "# # for k in np.arange(180)\n",
    "# data['taxi'].y=X_train_new[:, :, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "import torch.nn as nn\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data=data.to(device)\n",
    "Model_GNN=Model_GNN.to(device)\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = Model_GNN(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(Model_GNN.parameters(), lr=0.005, weight_decay=0.001)\n",
    "           \n",
    "def train(Model_GNN,data):\n",
    "    Model_GNN.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = Model_GNN(data.x_dict, data.edge_index_dict)\n",
    "    mask = data['taxi'].train_mask\n",
    "#     loss = F.cross_entropy(out[mask], data['domain_node'].y[mask])\n",
    "    loss = mse_loss(out[mask], data['taxi'].y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test(model,data):\n",
    "#     model.eval()\n",
    "#     pred = model(data.x_dict, data.edge_index_dict).argmax(dim=-1)\n",
    "#     accs = []\n",
    "#     for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "#         mask = data[split]\n",
    "#         acc = (pred[mask] == data['domain_node'].y[mask]).sum() / mask.sum()\n",
    "#         accs.append(float(acc))\n",
    "#     return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994780793319415\n",
      "0.9994780793319415\n",
      "0.9994780793319415\n",
      "0.9994780793319415\n",
      "0.9994780793319415\n",
      "0.9994780793319415\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b15adf867e14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_graph_per_ts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtemp2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'taxi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'taxi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-3d04fedd7585>\u001b[0m in \u001b[0;36mget_graph_per_ts\u001b[0;34m(t_s)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#     print(A.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0medge_list0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA_to_edge_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0medge_list1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA_to_edge_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0medge_list2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA_to_edge_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-11c2aa8a7d0e>\u001b[0m in \u001b[0;36mA_to_edge_index\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this routine converts adjacency matrices into adjacency lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mA_to_edge_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/networkx/convert_matrix.py\u001b[0m in \u001b[0;36mfrom_numpy_matrix\u001b[0;34m(A, parallel_edges, create_using)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_multigraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mtriples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtriples\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36madd_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 180):\n",
    "    data=get_graph_per_ts(0)\n",
    "    temp=np.array([True])\n",
    "    temp2=np.tile(temp, data['taxi'].x.shape[0])\n",
    "    data['taxi'].train_mask=torch.from_numpy(temp2)\n",
    "    data=data.to(device)\n",
    "    loss = train(Model_GNN, data)\n",
    "    if epoch % 20 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(1, 179):\n",
    "#     data=get_graph_per_ts(1)\n",
    "#     temp=np.array([True])\n",
    "#     temp2=np.tile(temp, data['taxi'].x.shape[0])\n",
    "#     data['taxi'].train_mask=torch.from_numpy(temp2)\n",
    "#     data=data.to(device)\n",
    "#     loss = train(Model_GNN, data)\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "lossl1 = nn.L1Loss()\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_per_ts(t_s):\n",
    "# obtain data for the cars in the first 10 regions\n",
    "    arr = np.empty((0,216))\n",
    "    arr_tru=np.empty((0,216))\n",
    "#     t_s=0\n",
    "    for i in range(40):\n",
    "        n_nodes_in_reg=np.sum(data1[t_s, i])\n",
    "\n",
    "        message_nodes_in_reg=X_train_new[i,:, t_s]\n",
    "        X_temp=np.matlib.repmat(message_nodes_in_reg, n_nodes_in_reg, 1)\n",
    "        arr = np.vstack((arr, X_temp))\n",
    "\n",
    "        message_nodes_in_reg_tru=X_train_new[i,:,t_s+1]\n",
    "        X_temp2=np.matlib.repmat(message_nodes_in_reg_tru, n_nodes_in_reg, 1)\n",
    "        arr_tru = np.vstack((arr_tru, X_temp2))\n",
    "\n",
    "    # X=np.asarray(X)   \n",
    "#     print(arr.shape)\n",
    "#     print(arr_tru.shape)  \n",
    "\n",
    "\n",
    "    # obtain adj info for the cars in the first 10 regions\n",
    "#     t_s=0\n",
    "    n_nodes_in_reg_list=[]\n",
    "    A0 = np.empty((0,0))\n",
    "    A1 = np.empty((0,0))\n",
    "    A2 = np.empty((0,0))\n",
    "    for i in range(40):\n",
    "            n_nodes_in_reg=np.sum(data1[t_s, i])\n",
    "            block0=np.matlib.repmat(adj_0[i,i], n_nodes_in_reg, n_nodes_in_reg);\n",
    "            A0=block_diag(A0, block0)\n",
    "            \n",
    "            block1=np.matlib.repmat(adj_0[i,i], n_nodes_in_reg, n_nodes_in_reg);\n",
    "            A1=block_diag(A1, block1)\n",
    "            \n",
    "            block2=np.matlib.repmat(adj_0[i,i], n_nodes_in_reg, n_nodes_in_reg);\n",
    "            A2=block_diag(A2, block2)\n",
    "    \n",
    "#     A0=A0.astype(float) \n",
    "#     print(A.shape)\n",
    "\n",
    "    edge_list0=A_to_edge_index(A0)\n",
    "    edge_list1=A_to_edge_index(A1)\n",
    "    edge_list2=A_to_edge_index(A2)\n",
    "#     print(edge_list0.shape)\n",
    "#     print(edge_list1.shape)\n",
    "#     print(edge_list2.shape)\n",
    "    data = HeteroData()\n",
    "    \n",
    "    \n",
    "    arr=torch.from_numpy(arr)\n",
    "#     mean1, std1 =arr.mean(axis=0), arr.std(axis=0)\n",
    "    data['taxi'].x =  (arr - 0) / 1\n",
    "    \n",
    "    \n",
    "    arr_tru=torch.from_numpy(arr_tru)\n",
    "#     mean2, std2 =arr_tru.mean(axis=0), arr_tru.std(axis=0)\n",
    "    data['taxi'].y =  (arr_tru - 0) / 1\n",
    "    \n",
    "    \n",
    "#     data['taxi'].x= arr\n",
    "    data['taxi', 'near', 'taxi'].edge_index = edge_list0\n",
    "    data['taxi', 'connected', 'taxi'].edge_index = edge_list1\n",
    "    data['taxi', 'OD_similar', 'taxi'].edge_index = edge_list2\n",
    "    temp=np.array([True])\n",
    "    temp2=np.tile(temp, 266)\n",
    "    data['taxi'].test_mask=torch.from_numpy(temp2)\n",
    "#     data['taxi'].y=torch.from_numpy(arr_tru)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data=get_graph_per_ts(144)\n",
    "temp=np.array([True])\n",
    "temp2=np.tile(temp, data['taxi'].x.shape[0])\n",
    "data['taxi'].test_mask=torch.from_numpy(temp2)\n",
    "data=data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1, std1 =data['taxi'].x.mean(axis=0), data['taxi'].x.std(axis=0)\n",
    "data['taxi'].x =  (data['taxi'].x - mean1) / std1\n",
    "# X_test_new[:, :, 1] =  (X_test_new[:, :, 1]  - mean1) / std1\n",
    "# # test_array = (data_array[(num_train + num_val) :] - mean) / std\n",
    "\n",
    "# data['taxi'].x= X_test_new[:, :, 0] \n",
    "# # for k in np.arange(180)\n",
    "# data['taxi'].y=X_test_new[:, :, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "#     Model_GNN.eval()\n",
    "    out = Model_GNN(data.x_dict, data.edge_index_dict)  \n",
    "    \n",
    "    \n",
    "# out[out<0]=0 \n",
    "estim=out.cpu()\n",
    "estim=(estim*std1.cpu()+mean1.cpu())\n",
    "\n",
    "org=data['taxi'].y.cpu()\n",
    "# org=(org+mean1)\n",
    "\n",
    "print(estim)\n",
    "print(org)\n",
    "rmse=math.sqrt(mse(estim, org))\n",
    "print(\"rmse\", rmse)\n",
    "print(\"mae\", lossl1(estim, org))\n",
    "print(\"mape\", mean_absolute_percentage_error(estim, org))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean1, std1 = X_test_new[:, :, 0].mean(axis=0), X_test_new[:, :, 0] .std(axis=0)\n",
    "# X_test_new[:, :, 0] =  (X_test_new[:, :, 0]  - mean1) / std1\n",
    "# X_test_new[:, :, 1] =  (X_test_new[:, :, 1]  - mean1) / std1\n",
    "# # test_array = (data_array[(num_train + num_val) :] - mean) / std\n",
    "\n",
    "# data['taxi'].x= X_test_new[:, :, 0] \n",
    "# # for k in np.arange(180)\n",
    "# data['taxi'].y=X_test_new[:, :, 1] \n",
    "data=data.to(device)\n",
    "Model_GNN=Model_GNN.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    Model_GNN.eval()\n",
    "    out = Model_GNN(data.x_dict, data.edge_index_dict)  \n",
    "\n",
    "# out[out<0]=0 \n",
    "# estim=out.cpu()\n",
    "# estim=(estim+mean1)\n",
    "\n",
    "# org=data['taxi'].y.cpu()\n",
    "# org=(org+mean1)\n",
    "\n",
    "# print(estim)\n",
    "# print(org)\n",
    "# rmse=math.sqrt(mse(estim, org))\n",
    "# print(\"rmse\", rmse)\n",
    "# print(\"mae\", lossl1(estim, org))\n",
    "# print(\"mape\", mean_absolute_percentage_error(estim, org))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    Model_GNN.eval()\n",
    "    out = Model_GNN(data.x_dict, data.edge_index_dict)  \n",
    "\n",
    "out[out<0]=0 \n",
    "estim=out.cpu()\n",
    "estim=(estim*std1.cpu()+mean1.cpu())\n",
    "\n",
    "org=data['taxi'].y.cpu()\n",
    "org=(org*std1+mean1)\n",
    "\n",
    "print(estim)\n",
    "print(org)\n",
    "rmse=math.sqrt(mse(estim, org))\n",
    "print(rmse)\n",
    "print(mean_absolute_percentage_error(estim, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_arr = [] \n",
    "# Y_arr = []\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(0,10):\n",
    "#     list1 = []\n",
    "#     for j in range(i,i+10):\n",
    "#         list1.append(X_test_new[j, :, 0])\n",
    "#     X.append(list1)\n",
    "#     Y.append(X_test_new[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test_new[:, :, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train test split\n",
    "# # X = np.array(X)\n",
    "# # Y = np.array(Y)\n",
    "# x_train = X_test_new[0:10,:,0]\n",
    "# y_train = X_test_new[10:11,:,1]\n",
    "# x_test = X_test_new[20:21,:,0]\n",
    "# y_test = X_test_new[20:21,:,1]\n",
    "# # y_train = Y[:360]\n",
    "# # y_test = Y[360:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train=np.array(np.transpose(x_train))\n",
    "# y_train=np.array(np.transpose(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataset\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class timeseries(Dataset):\n",
    "#     def __init__(self,x,y):\n",
    "#         self.x = torch.tensor(x,dtype=torch.float32)\n",
    "#         self.y = torch.tensor(y,dtype=torch.float32)\n",
    "#         self.len = x.shape[0]\n",
    "\n",
    "#     def __getitem__(self,idx):\n",
    "#         return self.x[idx],self.y[idx]\n",
    "  \n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "\n",
    "# dataset = timeseries(x_train,y_train)\n",
    "# #dataloader\n",
    "# from torch.utils.data import DataLoader \n",
    "# train_loader = DataLoader(dataset,shuffle=True,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #neural network\n",
    "# from torch import nn\n",
    "\n",
    "# class neural_network(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(neural_network,self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=216,hidden_size=5)\n",
    "#         self.fc1 = nn.Linear(in_features=5,out_features=216)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         output,_status = self.lstm(x)\n",
    "#         output = output[:,-1,:]\n",
    "#         output = self.fc1(torch.relu(output))\n",
    "#         return output\n",
    "\n",
    "# model = neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optimizer , loss\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data[:][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training loop\n",
    "# for i in range(epochs):\n",
    "#     for j,data in enumerate(train_loader):\n",
    "#         y_pred = model(data[:][0].view(-1,1,1)).reshape(-1)\n",
    "#         loss = criterion(y_pred,data[:][1])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     if i%50 == 0:\n",
    "#         print(i,\"th iteration : \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     input_size - will be 1 in this example since we have only 1 predictor (a sequence of previous values)\n",
    "#     hidden_size - Can be chosen to dictate how much hidden \"long term memory\" the network will have\n",
    "#     output_size - This will be equal to the prediciton_periods input to get_x_y_pairs\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "        \n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        \n",
    "#         self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "#     def forward(self, x, hidden=None):\n",
    "#         if hidden==None:\n",
    "#             self.hidden = (torch.zeros(1,1,self.hidden_size),\n",
    "#                            torch.zeros(1,1,self.hidden_size))\n",
    "#         else:\n",
    "#             self.hidden = hidden\n",
    "            \n",
    "#         \"\"\"\n",
    "#         inputs need to be in the right shape as defined in documentation\n",
    "#         - https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        \n",
    "#         lstm_out - will contain the hidden states from all times in the sequence\n",
    "#         self.hidden - will contain the current hidden state and cell state\n",
    "#         \"\"\"\n",
    "#         lstm_out, self.hidden = self.lstm(x.view(len(x),1,-1), \n",
    "#                                           self.hidden)\n",
    "        \n",
    "#         predictions = self.linear(lstm_out.view(len(x), -1))\n",
    "        \n",
    "#         return predictions[-1], self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTM(input_size=1, hidden_size=50, output_size=1)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 5\n",
    "# model.train()\n",
    "# for epoch in range(epochs+1):\n",
    "#     for x,y in zip(x_train, y_train):\n",
    "#         y_hat, _ = model(x, None)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = criterion(y_hat, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_LSTM = LSTM(input_size=1, hidden_size=50, output_size=12*9)\n",
    "# # criterion = nn.MSELoss()\n",
    "# # optimizer = optim.Adam(model_LSTM.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take the message from the GMM model:\n",
    "# with torch.no_grad():  # Initialize lazy modules.\n",
    "#      out_GNN = Model_GNN(data.x_dict, data.edge_index_dict)\n",
    "# print(out_GNN.shape)\n",
    "# GT=data['taxi'].y\n",
    "# print(GT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with the GNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully decentralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data.x_dict=data.x_dict+10*np.random.randn((266,216))    \n",
    "aa=data.x_dict['taxi']\n",
    "noise = 5+1*np.random.normal(0, 1, aa.shape)\n",
    "bb=aa.cpu()\n",
    "data.x_dict['taxi']=bb\n",
    "print(data.x_dict['taxi']-aa)\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "#     Model_GNN.eval()\n",
    "    out = Model_GNN(data.x_dict, data.edge_index_dict)  \n",
    "    \n",
    "    \n",
    "out[out<0]=0 \n",
    "estim=out.cpu()\n",
    "estim=(estim+mean1)\n",
    "\n",
    "org=data['taxi'].y.cpu()+0*noise\n",
    "org=(org+mean1)\n",
    "\n",
    "print(estim)\n",
    "print(org)\n",
    "rmse=math.sqrt(mse(estim, org))\n",
    "print(rmse)\n",
    "print(lossl1(estim, org))\n",
    "print(mean_absolute_percentage_error(estim, org))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aa.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
